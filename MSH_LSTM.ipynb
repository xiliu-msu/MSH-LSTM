{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "set_idx = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import Ipynb_importer\n",
    "import torch\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import numpy as np\n",
    "import scipy.io as sio  \n",
    "import matplotlib.pyplot as plt  \n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import copy as copy\n",
    "\n",
    "from SST_loader import SST_loader\n",
    "from res_disp import res_disp\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "# Settings\n",
    "cuda = True\n",
    "flag_window = True\n",
    "window_sz = 6\n",
    "flag_autoregress = False\n",
    "feature_opt = 'mean'\n",
    "perc_dim = []\n",
    "\n",
    "if set_idx == 1:\n",
    "    idx_train = np.array(range(0,292))\n",
    "    idx_valid = np.array(range(300,320))\n",
    "    idx_test = np.array(range(328,348))\n",
    "elif set_idx == 2:\n",
    "    idx_train = np.concatenate((np.array(range(0,272)),np.array(range(336,348))),0)\n",
    "    idx_valid = np.array(range(280,300))\n",
    "    idx_test = np.array(range(308,328))\n",
    "elif set_idx == 3:\n",
    "    idx_train = np.concatenate((np.array(range(0,252)),np.array(range(316,348))),0)\n",
    "    idx_valid = np.array(range(260,280))\n",
    "    idx_test = np.array(range(288,308))\n",
    "elif set_idx == 4:\n",
    "    idx_train = np.concatenate((np.array(range(0,232)),np.array(range(296,348))),0)\n",
    "    idx_valid = np.array(range(240,260))\n",
    "    idx_test = np.array(range(268,288))\n",
    "elif set_idx == 5:\n",
    "    idx_train = np.concatenate((np.array(range(0,212)),np.array(range(276,348))),0)\n",
    "    idx_valid = np.array(range(220,240))\n",
    "    idx_test = np.array(range(248,268))\n",
    "elif set_idx == 6:\n",
    "    idx_train = np.concatenate((np.array(range(0,192)),np.array(range(256,348))),0)\n",
    "    idx_valid = np.array(range(200,220))\n",
    "    idx_test = np.array(range(228,248))\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not flag_window and window_sz!=0:\n",
    "    print('!!!')\n",
    "\n",
    "n_stn = 58\n",
    "flag = True\n",
    "for i in range(n_stn):\n",
    "    if flag:\n",
    "        temp_X,temp_y,_ = SST_loader('../data/multimodel-SST_raw_'+str(i+1)+'.mat', idx_train, feature_opt, flag_window, window_sz, flag_autoregress, perc_dim)\n",
    "        X_train = temp_X[idx_train,:,:,:,:]\n",
    "        y_train = temp_y[idx_train,:,:]\n",
    "        \n",
    "        X_valid = temp_X[idx_valid,:,:,:,:]\n",
    "        y_valid = temp_y[idx_valid,:,:]\n",
    "        \n",
    "        X_test = temp_X[idx_test,:,:,:,:]\n",
    "        y_test = temp_y[idx_test,:,:]\n",
    "\n",
    "        flag = False\n",
    "    else:\n",
    "        temp_X,temp_y,_ = SST_loader('../data/multimodel-SST_raw_'+str(i+1)+'.mat', idx_train, feature_opt, flag_window, window_sz, flag_autoregress, perc_dim)\n",
    "        X_train = np.concatenate((X_train,temp_X[idx_train,:,:,:,:]),0)\n",
    "        y_train = np.concatenate((y_train,temp_y[idx_train,:,:]),0)\n",
    "\n",
    "        X_valid = np.concatenate((X_valid,temp_X[idx_valid,:,:,:,:]),0)\n",
    "        y_valid = np.concatenate((y_valid,temp_y[idx_valid,:,:]),0)\n",
    "\n",
    "        X_test = np.concatenate((X_test,temp_X[idx_test,:,:,:,:]),0)\n",
    "        y_test = np.concatenate((y_test,temp_y[idx_test,:,:]),0)\n",
    "\n",
    "\n",
    "_,_,n_lead,n_model,n_dim = X_train.shape\n",
    "\n",
    "\n",
    "if flag_autoregress:\n",
    "    n_model = n_model-1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = torch.from_numpy(X_train).float()\n",
    "y_train = torch.from_numpy(y_train).float()\n",
    "X_valid = torch.from_numpy(X_valid).float()\n",
    "y_valid = torch.from_numpy(y_valid).float()\n",
    "X_test = torch.from_numpy(X_test).float()\n",
    "y_test = torch.from_numpy(y_test).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class hierarchy_LSTM(nn.Module):\n",
    "\n",
    "    def __init__(self, n_lead, flag_autoregress, n_dim, para):\n",
    "        super(hierarchy_LSTM, self).__init__()\n",
    "        self.n_lead = n_lead\n",
    "        self.flag_autoregress = flag_autoregress\n",
    "        self.n_dim = n_dim\n",
    "        self.hidden_dim_bot = para.hidden_dim_bot\n",
    "        self.hidden_dim_top = para.hidden_dim_top\n",
    "        self.dp_rate_out = para.dp_rate_out\n",
    "\n",
    "\n",
    "        self.relu = nn.ReLU()\n",
    "    \n",
    "        \n",
    "        ## 1st level lstm\n",
    "        self.lstm_bot = nn.LSTM(input_size=self.n_dim, hidden_size=self.hidden_dim_bot, num_layers=1, bidirectional=False)\n",
    "\n",
    "        \n",
    "        ## 2nd level lstm\n",
    "        self.lstm_top = nn.ModuleList([])\n",
    "        for l in range(self.n_lead):\n",
    "            if self.flag_autoregress:\n",
    "                self.lstm_top.append(nn.LSTM(input_size=(self.hidden_dim_bot)+1, hidden_size=self.hidden_dim_top, num_layers=1, bidirectional=False))\n",
    "            else:\n",
    "                self.lstm_top.append(nn.LSTM(input_size=(self.hidden_dim_bot), hidden_size=self.hidden_dim_top, num_layers=1, bidirectional=False))\n",
    "\n",
    "\n",
    "        ## output layer\n",
    "        self.fc1 = nn.ModuleList([])\n",
    "        for l in range(self.n_lead):\n",
    "            self.fc1.append(nn.Linear(self.hidden_dim_top, 1))\n",
    "        self.out_dropout = nn.Dropout(p=self.dp_rate_out)\n",
    "\n",
    "\n",
    "\n",
    "    def init_hidden(self,batch_size,hidden_dim,cuda): # 1 hidden, 2 cell\n",
    "        if cuda:\n",
    "            return (Variable(torch.zeros(1, batch_size, hidden_dim).cuda()),\n",
    "                    Variable(torch.zeros(1, batch_size, hidden_dim).cuda()))\n",
    "        else:\n",
    "            return (Variable(torch.zeros(1, batch_size, hidden_dim)),\n",
    "                    Variable(torch.zeros(1, batch_size, hidden_dim)))\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    def forward(self, inputs_all, cuda):\n",
    "        batch_size = inputs_all.size()[0]\n",
    "        window_sz = inputs_all.size()[1]\n",
    "          \n",
    "        if self.flag_autoregress:\n",
    "            inputs = inputs_all[:,:,:,0:-1,:]\n",
    "        else:\n",
    "            inputs = inputs_all\n",
    "            \n",
    "         \n",
    "        x = torch.squeeze(inputs).contiguous()\n",
    "        \n",
    "        ## 1st level lstm\n",
    "        x = x.view(-1,self.n_lead,self.n_dim)  # (n*seq_top, seq_bot, embed_dim)\n",
    "        x = x.permute(1,0,2)   # (seq_bot, n*seq_top, embed_dim)\n",
    "        hidden_bot = self.init_hidden(batch_size*window_sz,self.hidden_dim_bot,cuda)\n",
    "        lstm_bot_out, hidden_bot = self.lstm_bot(x, hidden_bot)  # (seq_bot, n*seq_top, dim_bot)\n",
    "        hidden_bot = (hidden_bot[0].detach(),hidden_bot[1].detach())\n",
    "        lstm_bot_out_org = lstm_bot_out.permute(1,0,2).contiguous()   #( n*seq_top, seq_bot, dim_bot)\n",
    "        lstm_bot_out_org = lstm_bot_out_org.view(batch_size, window_sz, self.n_lead, self.hidden_dim_bot)         #( n, seq_top, seq_bot, dim_bot)\n",
    "\n",
    "        if self.flag_autoregress:\n",
    "            latest_obs = inputs_all[:,:,:,-1,0].contiguous()\n",
    "            latest_obs = latest_obs.view(batch_size,window_sz,self.n_lead,1)\n",
    "            context = torch.cat((lstm_bot_out_org,latest_obs),3)\n",
    "        else:\n",
    "            context = lstm_bot_out_org\n",
    "        #( n, seq_top, seq_bot, dim_bot)  \n",
    "            \n",
    "            \n",
    "            \n",
    "            \n",
    "        ## 2nd level lstm\n",
    "        if cuda:\n",
    "            lstm_top_out = Variable(torch.zeros(window_sz,batch_size,self.n_lead,self.hidden_dim_top)).cuda()\n",
    "        else:\n",
    "            lstm_top_out = Variable(torch.zeros(window_sz,batch_size,self.n_lead,self.hidden_dim_top)) #( seq_top, n, seq_bot, dim_top) \n",
    "        context = context.permute(1,0,2,3)   #( seq_top, n, seq_bot, dim_bot) \n",
    "        for l in range(self.n_lead):\n",
    "            hidden_top = self.init_hidden(batch_size,self.hidden_dim_top,cuda)\n",
    "            lstm_top_out[:,:,l,:], hidden_top = self.lstm_top[l](torch.squeeze(context[:,:,l,:]), hidden_top)\n",
    "            hidden_top = (hidden_top[0].detach(),hidden_top[1].detach())\n",
    "        lstm_top_out_org = lstm_top_out.permute(1,0,2,3)   #( n, seq_top, seq_bot, dim_top) \n",
    "\n",
    "        ## output layer\n",
    "        if cuda:\n",
    "            out = Variable(torch.zeros(batch_size,window_sz,self.n_lead)).cuda()\n",
    "        else:\n",
    "            out = Variable(torch.zeros(batch_size,window_sz,self.n_lead))\n",
    "        for l in range(self.n_lead):\n",
    "            temp = self.out_dropout(lstm_top_out_org)\n",
    "            temp = self.fc1[l](temp[:,:,l,:])\n",
    "            out[:,:,l] = temp\n",
    "        # (n, seq_top, seq_bot)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(dataloader,net,cuda):\n",
    "    all_outputs = torch.zeros(0,n_lead)\n",
    "    all_targets = torch.zeros(0,n_lead)\n",
    "    for i, data in enumerate(dataloader):\n",
    "        inputs_batch, targets_batch = data\n",
    "        all_targets = torch.cat((all_targets,targets_batch[:,-1,:]),0)\n",
    "        inputs_batch = Variable(torch.squeeze(inputs_batch),requires_grad=False)\n",
    "        if cuda:\n",
    "            inputs_batch = inputs_batch.cuda()\n",
    "            all_outputs = torch.cat((all_outputs,net(inputs_batch,cuda).data.cpu()[:,-1,:]),0)\n",
    "        else:\n",
    "            all_outputs = torch.cat((all_outputs,net(inputs_batch,cuda).data[:,-1,:]),0)\n",
    "    return all_outputs, all_targets\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "class para_ele:\n",
    "    def __init__(self,batchsize,learn_rate,hidden_dim_bot,hidden_dim_top,dp_rate_out):\n",
    "        self.batchsize = batchsize\n",
    "        self.learn_rate = learn_rate\n",
    "        self.hidden_dim_bot = hidden_dim_bot\n",
    "        self.hidden_dim_top = hidden_dim_top\n",
    "        self.dp_rate_out = dp_rate_out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "if set_idx == 1:\n",
    "    para_ele_list = [para_ele(256, 0.0010, 50, 40, 0.0000)]   # set 1 \n",
    "elif set_idx == 2:\n",
    "    para_ele_list = [para_ele(256, 0.0010, 10, 50, 0.0000)]   # set 2  \n",
    "elif set_idx == 3:\n",
    "    para_ele_list = [para_ele(128, 0.0010, 30, 10, 0.5000)]   # set 3\n",
    "elif set_idx == 4:\n",
    "    para_ele_list = [para_ele(128, 0.0100, 10, 10, 0.5000)]   # set 4\n",
    "elif set_idx == 5:\n",
    "    para_ele_list = [para_ele(128, 0.0010, 30, 10, 0.5000)]   # set 5\n",
    "elif set_idx == 6:\n",
    "    para_ele_list = [para_ele(256, 0.0100, 10, 10, 0.1000)]   # set 6 \n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "all_outputs_test = []\n",
    "all_targets_test = []\n",
    "\n",
    "seeds = range(15)\n",
    "for para in para_ele_list:\n",
    "\n",
    "    all_rmse_train = torch.zeros(n_lead,len(seeds))\n",
    "    all_rmse_valid = torch.zeros(n_lead,len(seeds))\n",
    "    all_rmse_test = torch.zeros(n_lead,len(seeds))\n",
    "    \n",
    "    n_trial = 0\n",
    "    \n",
    "    trainset = torch.utils.data.TensorDataset(X_train,y_train)\n",
    "    trainloader = torch.utils.data.DataLoader(trainset, batch_size=para.batchsize, shuffle=True)\n",
    "    validset = torch.utils.data.TensorDataset(X_valid,y_valid)\n",
    "    validloader = torch.utils.data.DataLoader(validset, batch_size=para.batchsize, shuffle=False)\n",
    "    testset = torch.utils.data.TensorDataset(X_test,y_test)\n",
    "    testloader = torch.utils.data.DataLoader(testset, batch_size=para.batchsize, shuffle=False)\n",
    "    \n",
    "    \n",
    "    \n",
    "    for seed in seeds:\n",
    "        n_trial += 1\n",
    "    \n",
    "        torch.manual_seed(seed)\n",
    "        net = hierarchy_LSTM( n_lead, flag_autoregress, n_dim, para )\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        optimizer = torch.optim.Adam(net.parameters(), lr = para.learn_rate)\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        if cuda:\n",
    "            net = net.cuda()\n",
    "            criterion = criterion.cuda()\n",
    "\n",
    "        best_loss = 1000000000000000000\n",
    "        best_net = copy.deepcopy(net)\n",
    "        stop_cr = 0\n",
    "\n",
    "        for epoch in range(5000):  # loop over the dataset multiple times\n",
    "            net.train()\n",
    "            running_loss = 0.0\n",
    "            for i, data in enumerate(trainloader):\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                inputs, targets = data\n",
    "                if cuda:\n",
    "                    inputs = inputs.cuda()\n",
    "                    targets = targets.cuda()\n",
    "                \n",
    "                outputs = net(Variable(inputs),cuda)   # (n, window_sz, n_lead)\n",
    "                \n",
    "                loss = criterion(outputs,Variable(targets))\n",
    "\n",
    "\n",
    "                loss.backward()            \n",
    "                optimizer.step()\n",
    "                running_loss += loss.data[0]\n",
    "\n",
    "            if epoch%1 == 0:\n",
    "                net.eval()\n",
    "                \n",
    "                output_valid, target_valid = predict(validloader,net,cuda)\n",
    "\n",
    "                valid_loss = torch.mean(torch.sqrt(torch.mean((output_valid-target_valid)**2,0)))\n",
    "\n",
    "                current_loss = valid_loss\n",
    "                if current_loss < best_loss:\n",
    "                    stop_cr = 0\n",
    "                    best_loss = current_loss\n",
    "                    best_net = copy.deepcopy(net)\n",
    "                else:\n",
    "                    stop_cr += 1\n",
    "                if stop_cr>=10:\n",
    "                    break\n",
    "\n",
    "        net = copy.deepcopy(best_net)\n",
    "        net.eval()\n",
    "        \n",
    "\n",
    "\n",
    "\n",
    "        # Testing Part\n",
    "        net.eval()\n",
    "               \n",
    "        outputs_train, targets_train = predict(trainloader,net,cuda)\n",
    "        outputs_valid, targets_valid = predict(validloader,net,cuda)\n",
    "        outputs_test, targets_test = predict(testloader,net,cuda)\n",
    "\n",
    "        all_rmse_train[:,n_trial-1] =  torch.sqrt(torch.mean((outputs_train-targets_train)**2,0))\n",
    "        all_rmse_valid[:,n_trial-1] = torch.sqrt(torch.mean((outputs_valid-targets_valid)**2,0))\n",
    "        all_rmse_test[:,n_trial-1] = torch.sqrt(torch.mean((outputs_test-targets_test)**2,0))\n",
    "        \n",
    "        \n",
    "        all_outputs_test.append(outputs_test.numpy())\n",
    "        all_targets_test.append(targets_test.numpy())\n",
    "\n",
    "    \n",
    "    ave_rmse_train = torch.mean(all_rmse_train,1)\n",
    "    ave_rmse_valid = torch.mean(all_rmse_valid,1)\n",
    "    ave_rmse_test = torch.mean(all_rmse_test,1)\n",
    "    std_rmse_train = torch.std(all_rmse_train,1)\n",
    "    std_rmse_valid = torch.std(all_rmse_valid,1)\n",
    "    std_rmse_test = torch.std(all_rmse_test,1)\n",
    "\n",
    "    print(para.batchsize, para.learn_rate, para.hidden_dim_bot, para.hidden_dim_top, para.dp_rate_out)\n",
    "    print('%.4f, %.4f, %.4f' % (torch.mean(ave_rmse_train),torch.mean(ave_rmse_valid),torch.mean(ave_rmse_test)))\n",
    "    print('-----------------------------------')\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
